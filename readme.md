# ğŸš€ Qwen3 RAG Pipeline

**Production-ready RAG system** with local inference, durable workflows, and full observability.

[
[

## âœ¨ Features
- **Qwen3:1.7B** - Lightweight, multilingual LLM via Ollama
- **GE-M3:567M** - Open-source embeddings (384-dim)
- **Qdrant** - Vector database with hybrid search
- **Inngest** - Durable workflows with step-level retries
- **FastAPI** - Production API with Swagger docs
- **Streamlit** - Interactive chat UI
- **LlamaIndex** - Advanced document chunking

## ğŸ—ï¸ Architecture
```
PDF â†’ LlamaIndex (chunk) â†’ GE-M3 Embed â†’ Qdrant â†’ Inngest RAG â†’ Qwen3 â†’ Answer
                           (FastAPI + Streamlit UI)
```

## ğŸš€ Quick Start

1. **Clone & Install**
```bash
git clone https://github.com/SHREE/rag-project.git
cd rag-project
conda create -n rag python=3.10
conda activate rag
pip install -r requirements.txt
```

2. **Start Services**
```bash
# Terminal 1: Ollama
ollama serve &
ollama pull qwen3:1.7b
ollama pull ge-m3:567m  # Embeddings

# Terminal 2: Qdrant
docker run -p 6333:6333 qdrant/qdrant

# Terminal 3: API
uvicorn main:app --reload
```

3. **Frontend**
```bash
streamlit run streamlit_app.py
```

## ğŸ“ API Endpoints
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/ingest` | Upload PDF for indexing |
| `POST` | `/query` | RAG query with sources |
| `GET` | `/docs` | Interactive Swagger UI |

**Test Query:**
```bash
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{"pdf_path": "sample.pdf"}'

curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG?", "top_k": 5}'
```

## ğŸ› ï¸ Project Structure
```
rag-project/
â”œâ”€â”€ main.py           # FastAPI + Inngest workflows
â”œâ”€â”€ vector_db.py      # Qdrant storage
â”œâ”€â”€ data_loader.py    # LlamaIndex chunking
â”œâ”€â”€ streamlit_app.py  # Chat UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example      # Copy to .env
â””â”€â”€ README.md
```

## ğŸ”§ Configuration
Copy `.env.example` â†’ `.env`:
```env
OLLAMA_BASE_URL=http://host.docker.internal:11434
QDRANT_URL=http://localhost:6333
EMBEDDING_DIM=384
```

## ğŸ“ˆ Monitoring
- **Inngest Dashboard**: Workflow traces, retries, metrics
- **FastAPI /metrics**: Prometheus endpoint
- **Ollama Logs**: `ollama ps`

## ğŸ” Troubleshooting
| Issue | Solution |
|-------|----------|
| Qdrant connection | `docker run -p 6333:6333 qdrant/qdrant` |
| Ollama 404 | `ollama serve & ollama pull qwen3:1.7b` |
| Embedding dim mismatch | Check `EMBEDDING_DIM=384` in `.env` |

## ğŸ¯ Why This Stack?
- **Local-first**: Zero API costs, full privacy
- **Durable**: Inngest retries failed embeddings/RAG steps
- **Scalable**: FastAPI async + Qdrant clustering-ready
- **Modern**: TypeScript-level DX with Pydantic + Inngest types

## ğŸ“„ License
MIT License - Use freely in commercial projects! [LICENSE](LICENSE)

## ğŸ™Œ Contributing
1. Fork & PR
2. Update tests: `pytest`
3. Follow PEP8: `black .`

**â­ Star if useful!** Questions? [Open issue](https://github.com/SHREE/rag-project/issues)
